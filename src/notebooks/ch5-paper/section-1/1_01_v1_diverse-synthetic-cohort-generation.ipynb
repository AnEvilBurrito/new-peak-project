{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9f153c",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d838737e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path set to: c:\\Github\\new-peak-project\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find(\"project\")\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[: index_project + 7]\n",
    "# set the working directory\n",
    "os.chdir(project_path + \"/src\")\n",
    "print(f\"Project path set to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a878b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983651e7",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14771320",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… S3 connection successful. Bucket: bio-data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_config.yml: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31k/1.31k [00:00<00:00, 10.3kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration saved to S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from models.utils.s3_config_manager import S3ConfigManager\n",
    "\n",
    "# Initialize S3 config manager\n",
    "s3_manager = S3ConfigManager()\n",
    "\n",
    "# Define notebook configuration following ch5-paper conventions\n",
    "notebook_name = 'diverse-synthetic-cohort-generation'\n",
    "exp_number = '01'  # 1st experiment in section\n",
    "section_number = '1'  # located in section-1\n",
    "version_number = 'v1'\n",
    "\n",
    "notebook_config = {\n",
    "    'exp_number': exp_number,\n",
    "    'version_number': version_number, \n",
    "    'notebook_name': notebook_name,\n",
    "    'section_number': section_number\n",
    "}\n",
    "\n",
    "# Calculate notebook path for S3 storage\n",
    "notebook_path = f'{exp_number}_{version_number}_{notebook_name}'\n",
    "\n",
    "# Define experiment configuration with full processing pipeline\n",
    "exp_config = {\n",
    "    'spec': {\n",
    "        'n_layers': 2,\n",
    "        'n_cascades': 3,\n",
    "        'n_regs': 0,\n",
    "        'gen_seed': 42,\n",
    "        'basal_activation': True,\n",
    "        'custom_regulations': [\n",
    "            ['R1', 'R2', 'up'],\n",
    "            ['R3', 'I1_2', 'up'],\n",
    "            ['I1_1', 'I2_2', 'up'],\n",
    "            ['I1_2', 'I2_1', 'down'],\n",
    "            ['I1_2', 'I2_3', 'down'],\n",
    "            ['I1_3', 'I2_2', 'up'],\n",
    "            ['I2_1', 'R1', 'down'],\n",
    "            ['I2_3', 'R3', 'up']\n",
    "        ],\n",
    "        'drug': {\n",
    "            'name': \"D\",\n",
    "            'start': 500,\n",
    "            'dose': 500,\n",
    "            'regulations': [\n",
    "                ['R1', 'down']\n",
    "            ],\n",
    "            'target_all': False\n",
    "        }\n",
    "    },\n",
    "    'parameter_generation': {\n",
    "        'ic_range': [200, 1000],\n",
    "        'param_range': [0.5, 2],\n",
    "        'param_mul_range': [0.99, 1.01]\n",
    "    },\n",
    "    'parameter_sampling': {\n",
    "        'sampling_seed': 42,\n",
    "        'num_models': 1000,\n",
    "        'num_datapoints': 1000\n",
    "    },\n",
    "    'feature_generation': {\n",
    "        'include_parameters': False,  # kinetic parameters also used as feature data\n",
    "        'excluded_layers': ['O'],\n",
    "        'perturbation_type': 'lhs',\n",
    "        'exclude_active_form': True\n",
    "    },\n",
    "    'simulation': {\n",
    "        'start': 0,\n",
    "        'stop': 1000,\n",
    "        'step': 100\n",
    "    },\n",
    "    'dynamic_data': {\n",
    "        'exclude_activated_form': False,\n",
    "        'excluded_layers': [],\n",
    "        'distortion': True,\n",
    "        'distortion_factor': 2\n",
    "    },\n",
    "    'machine_learning': {\n",
    "        'ml_seed': 42,\n",
    "        'outcome_var': 'Oa',\n",
    "        'n_samples': 1000,  # number of samples used\n",
    "        'n_reps': 10\n",
    "    }\n",
    "}\n",
    "\n",
    "# Combine configurations\n",
    "full_config = {\n",
    "    'notebook': notebook_config,\n",
    "    'exp': exp_config\n",
    "}\n",
    "\n",
    "# Save configuration to S3 using version number as config suffix\n",
    "s3_manager.save_config(notebook_config, full_config, config_suffix=version_number)\n",
    "print(\"âœ… Configuration saved to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64c10f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading v1_config.yml:   0%|          | 0.00/1.31k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading v1_config.yml: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31k/1.31k [00:00<00:00, 27.2kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook: \n",
      "  exp_number: 01\n",
      "  version_number: v1\n",
      "  notebook_name: diverse-synthetic-cohort-generation\n",
      "  section_number: 1\n",
      "exp: \n",
      "  spec: \n",
      "    n_layers: 2\n",
      "    n_cascades: 3\n",
      "    n_regs: 0\n",
      "    gen_seed: 42\n",
      "    basal_activation: True\n",
      "    custom_regulations: [['R1', 'R2', 'up'], ['R3', 'I1_2', 'up'], ['I1_1', 'I2_2', 'up'], ['I1_2', 'I2_1', 'down'], ['I1_2', 'I2_3', 'down'], ['I1_3', 'I2_2', 'up'], ['I2_1', 'R1', 'down'], ['I2_3', 'R3', 'up']]\n",
      "    drug: \n",
      "      name: D\n",
      "      start: 500\n",
      "      dose: 500\n",
      "      regulations: [['R1', 'down']]\n",
      "      target_all: False\n",
      "  parameter_generation: \n",
      "    ic_range: [200, 1000]\n",
      "    param_range: [0.1, 10]\n",
      "    param_mul_range: [0.99, 1.01]\n",
      "  parameter_sampling: \n",
      "    sampling_seed: 42\n",
      "    num_models: 1000\n",
      "    num_datapoints: 1000\n",
      "  feature_generation: \n",
      "    include_parameters: False\n",
      "    excluded_layers: ['O']\n",
      "    perturbation_type: lhs\n",
      "    exclude_active_form: True\n",
      "  simulation: \n",
      "    start: 0\n",
      "    stop: 1000\n",
      "    step: 100\n",
      "  dynamic_data: \n",
      "    exclude_activated_form: False\n",
      "    excluded_layers: []\n",
      "    distortion: True\n",
      "    distortion_factor: 2\n",
      "  machine_learning: \n",
      "    ml_seed: 42\n",
      "    outcome_var: Oa\n",
      "    n_samples: 1000\n",
      "    n_reps: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# or load existing config using version number as config suffix\n",
    "loaded_config = s3_manager.load_config(notebook_config, config_suffix=version_number)\n",
    "\n",
    "# Print configuration for verification\n",
    "def print_config(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print(\" \" * indent + str(key) + \":\", end=\" \")\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            print_config(value, indent + 2)\n",
    "        else:\n",
    "            print(str(value))\n",
    "\n",
    "print_config(loaded_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0b38ad",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a6e87",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0adc74d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from models.Specs.ModelSpec4 import ModelSpec4\n",
    "\n",
    "notebook_config = loaded_config[\"notebook\"]\n",
    "config_name = notebook_config['version_number']\n",
    "exp_config = loaded_config[\"exp\"]\n",
    "spec_config = exp_config['spec']\n",
    "n_layers = spec_config['n_layers']\n",
    "new_spec = ModelSpec4(num_intermediate_layers=n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f40585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Specs.Drug import Drug\n",
    "\n",
    "drug_config = spec_config['drug']\n",
    "drug_name = drug_config['name']\n",
    "drug_start = drug_config['start']\n",
    "drug_dose = drug_config['dose']\n",
    "drug_regulations = drug_config['regulations']\n",
    "\n",
    "\n",
    "n_cascades = spec_config[\"n_cascades\"]\n",
    "n_regs = spec_config[\"n_regs\"]\n",
    "seed = spec_config[\"gen_seed\"]\n",
    "\n",
    "for regulation in spec_config[\"custom_regulations\"]:\n",
    "    new_spec.add_regulation(*regulation)\n",
    "\n",
    "new_drug = Drug(name=drug_name, start_time=drug_start, default_value=drug_dose)\n",
    "\n",
    "# check if target_all exists in drug_config, if not set to False\n",
    "drug_target_all = drug_config.get('target_all', False)\n",
    "\n",
    "if drug_target_all:\n",
    "    # If the drug targets all receptors, we don't need to add specific regulations\n",
    "    for n in range(n_cascades):\n",
    "        target = f'R{n+1}' # assuming receptors are named R1, R2, ..., Rn\n",
    "        new_drug.add_regulation(target, 'down') # assuming the type is 'down' for all\n",
    "else: \n",
    "    for regs in drug_regulations:\n",
    "        target, type = regs[0], regs[1]\n",
    "        new_drug.add_regulation(target, type)\n",
    "\n",
    "new_spec.generate_specifications(n_cascades, n_regs, seed)\n",
    "new_spec.add_drug(new_drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fc323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "param_sampling_config = exp_config[\"parameter_sampling\"]\n",
    "base_sampling_seed = param_sampling_config[\"sampling_seed\"]\n",
    "num_models = param_sampling_config[\"num_models\"]\n",
    "# based on base seed create 1000 different seeds\n",
    "seeds = np.random.default_rng(base_sampling_seed).integers(0, 1000000, size=num_models)\n",
    "\n",
    "param_gen_config = exp_config['parameter_generation']\n",
    "basal_activation = spec_config[\"basal_activation\"]\n",
    "specie_range = param_gen_config['ic_range']\n",
    "param_range = param_gen_config['param_range']\n",
    "param_mul_range = param_gen_config['param_mul_range']\n",
    "\n",
    "builder_models = []\n",
    "for seed in seeds:\n",
    "    builder = new_spec.generate_network(\n",
    "        config_name,\n",
    "        specie_range,\n",
    "        param_range,\n",
    "        param_mul_range,\n",
    "        seed,\n",
    "        receptor_basal_activation=basal_activation,\n",
    "    )\n",
    "    builder_models.append(builder)\n",
    "print(f\"Generated {len(builder_models)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e250801",
   "metadata": {},
   "source": [
    "### Simulations with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95878f3d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No valid cached results found (Key not found: new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_sim_results_df.pkl), generating new simulations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating solvers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:51<00:00,  5.83it/s]\n",
      "Simulating models: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:03<00:00, 307.67it/s]\n",
      "Uploading v1_sim_results_df.pkl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.6M/17.6M [00:01<00:00, 14.0MB/s]\n",
      "Uploading v1_sim_results.pkl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.1M/17.1M [00:01<00:00, 16.7MB/s]\n",
      "Uploading v1_sim_results_df.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.7M/19.7M [00:00<00:00, 30.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… sim_results_df saved as parquet format\n",
      "âœ… New simulation results generated and cached\n"
     ]
    }
   ],
   "source": [
    "from models.Solver.RoadrunnerSolver import RoadrunnerSolver\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def validate_simulation_cache(sim_results_df, sim_results, expected_model_count):\n",
    "    \"\"\"\n",
    "    Validate that cached simulation results match expected structure\n",
    "    \"\"\"\n",
    "    if len(sim_results) != expected_model_count:\n",
    "        return False\n",
    "    if sim_results_df is None or len(sim_results_df) == 0:\n",
    "        return False\n",
    "    if 'seed' not in sim_results_df.columns:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def load_or_generate_simulation_results(notebook_config, builder_models, seeds, sim_config):\n",
    "    \"\"\"\n",
    "    Load existing simulation results if cached, otherwise generate new ones\n",
    "    Returns: sim_results_df, sim_results, solver\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to load existing cached results\n",
    "        sim_results_df = s3_manager.load_data(notebook_config, \"sim_results_df\", \"pkl\")\n",
    "        sim_results = s3_manager.load_data(notebook_config, \"sim_results\", \"pkl\")\n",
    "        \n",
    "        # Validate that loaded data matches expected dimensions\n",
    "        if validate_simulation_cache(sim_results_df, sim_results, len(builder_models)):\n",
    "            print(\"âœ… Loaded existing simulation results from cache\")\n",
    "            \n",
    "            # For cached results, create a minimal solver from the first model\n",
    "            solver = RoadrunnerSolver()\n",
    "            solver.compile(builder_models[0].get_sbml_model())\n",
    "            return sim_results_df, sim_results, solver\n",
    "        else:\n",
    "            print(\"âš ï¸ Cached results invalid, regenerating...\")\n",
    "            raise ValueError(\"Cache validation failed\")\n",
    "            \n",
    "    except (FileNotFoundError, ValueError, Exception) as e:\n",
    "        print(f\"âš ï¸ No valid cached results found ({e}), generating new simulations...\")\n",
    "        \n",
    "        # Create solvers\n",
    "        solver_models = []\n",
    "        for i, builder in enumerate(tqdm(builder_models, desc=\"Creating solvers\")):\n",
    "            solver = RoadrunnerSolver()\n",
    "            solver.compile(builder.get_sbml_model())\n",
    "            solver_models.append(solver)\n",
    "        \n",
    "        # Run simulations\n",
    "        sim_start = sim_config[\"start\"]\n",
    "        sim_stop = sim_config[\"stop\"]\n",
    "        sim_step = sim_config[\"step\"]\n",
    "        \n",
    "        sim_results = []\n",
    "        for i, solver in enumerate(tqdm(solver_models, desc=\"Simulating models\")):\n",
    "            res = solver.simulate(sim_start, sim_stop, sim_step)\n",
    "            sim_results.append(res)\n",
    "        \n",
    "        # Create dataframe\n",
    "        sim_results_df = pd.concat(\n",
    "            [pd.DataFrame(res).assign(seed=seed) for res, seed in zip(sim_results, seeds)],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        \n",
    "        # Cache for future use in both pickle and parquet formats\n",
    "        s3_manager.save_data(notebook_config, sim_results_df, \"sim_results_df\", \"pkl\")\n",
    "        s3_manager.save_data(notebook_config, sim_results, \"sim_results\", \"pkl\")\n",
    "        \n",
    "        # Also save sim_results_df in parquet format\n",
    "        try:\n",
    "            s3_manager.save_data(notebook_config, sim_results_df, \"sim_results_df\", \"parquet\")\n",
    "            print(\"âœ… sim_results_df saved as parquet format\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not save sim_results_df as parquet: {e}\")\n",
    "        \n",
    "        print(\"âœ… New simulation results generated and cached\")\n",
    "        \n",
    "        # Return the first solver for further processing\n",
    "        return sim_results_df, sim_results, solver_models[0]\n",
    "\n",
    "# Execute the caching simulation function\n",
    "sim_config = exp_config[\"simulation\"]\n",
    "sim_results_df, sim_results, simulation_solver = load_or_generate_simulation_results(\n",
    "    notebook_config, builder_models, seeds, sim_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7483d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_sim_results_df.pkl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.6M/17.6M [00:00<00:00, 27.5MB/s]\n",
      "Uploading v1_sim_results.pkl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.1M/17.1M [00:00<00:00, 22.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Simulation results saved to S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# save the results using S3 manager\n",
    "s3_manager.save_data(\n",
    "    notebook_config=notebook_config,\n",
    "    data=sim_results_df,\n",
    "    data_name=\"sim_results_df\",\n",
    "    data_format=\"pkl\",\n",
    ")\n",
    "\n",
    "s3_manager.save_data(\n",
    "    notebook_config=notebook_config,\n",
    "    data=sim_results,\n",
    "    data_name=\"sim_results\",\n",
    "    data_format=\"pkl\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Simulation results saved to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc6a22e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in S3:\n",
      "Config:\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/v1_config.yml\n",
      "Data:\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_sim_results.pkl\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_sim_results_df.parquet\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_sim_results_df.pkl\n"
     ]
    }
   ],
   "source": [
    "# Verify files are in S3\n",
    "files = s3_manager.list_experiment_files(notebook_config)\n",
    "print(\"Files in S3:\")\n",
    "for category, file_list in files.items():\n",
    "    if file_list:\n",
    "        print(f\"{category.capitalize()}:\")\n",
    "        for file in file_list:\n",
    "            print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793cdbc",
   "metadata": {},
   "source": [
    "# Bulk Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5d9a62",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from models.SyntheticGen import generate_feature_data_v3, generate_target_data_diff_build, generate_model_timecourse_data_diff_build_v3\n",
    "from models.utils.dynamic_calculations import dynamic_features_method, last_time_point_method\n",
    "from numpy.random import default_rng\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e916ac3",
   "metadata": {},
   "source": [
    "## Enhanced Feature Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02216be3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_enhanced_feature_data(model_spec, builder_models, exp_config):\n",
    "    \"\"\"\n",
    "    Generate feature data with optional parameter inclusion and layer exclusions\n",
    "    \"\"\"\n",
    "    feature_config = exp_config['feature_generation']\n",
    "    param_gen_config = exp_config['parameter_generation']\n",
    "    ml_config = exp_config['machine_learning']\n",
    "    \n",
    "    # Get initial values from first model with exclusions\n",
    "    builder = builder_models[0]  # Use first model as reference\n",
    "    initial_values = builder.get_state_variables()\n",
    "    \n",
    "    if feature_config.get('exclude_active_form', True):\n",
    "        initial_values = {k: v for k, v in initial_values.items() if not k.endswith('a')}\n",
    "    if feature_config.get('excluded_layers'):\n",
    "        for layer in feature_config['excluded_layers']:\n",
    "            initial_values = {k: v for k, v in initial_values.items() if not k.startswith(layer)}\n",
    "    \n",
    "    # Generate base feature data\n",
    "    feature_data = generate_feature_data_v3(\n",
    "        model_spec, initial_values, \n",
    "        feature_config['perturbation_type'],\n",
    "        {'min': param_gen_config['ic_range'][0], 'max': param_gen_config['ic_range'][1]},\n",
    "        ml_config['n_samples'],\n",
    "        seed=ml_config['ml_seed']\n",
    "    )\n",
    "    \n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7cb998",
   "metadata": {},
   "source": [
    "## Parameter Extraction from Builder Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07225baa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_parameter_sets_from_builders(builder_models):\n",
    "    \"\"\"\n",
    "    Extract parameter sets from all builder models for processing\n",
    "    \"\"\"\n",
    "    parameter_sets = []\n",
    "    for builder in builder_models:\n",
    "        params = builder.get_parameters()\n",
    "        parameter_sets.append(params)\n",
    "    return parameter_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353889a8",
   "metadata": {},
   "source": [
    "## Parameter Distortion for Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b09dc00e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def apply_parameter_distortion(parameter_sets, exp_config):\n",
    "    \"\"\"\n",
    "    Apply parameter distortion for robustness testing\n",
    "    \"\"\"\n",
    "    dynamic_config = exp_config['dynamic_data']\n",
    "    \n",
    "    if not dynamic_config.get('distortion', False):\n",
    "        return parameter_sets\n",
    "    \n",
    "    distortion_factor = dynamic_config['distortion_factor']\n",
    "    distort_range = (1 / distortion_factor, distortion_factor)\n",
    "    \n",
    "    ml_seed = exp_config['machine_learning']['ml_seed']\n",
    "    rng = default_rng(ml_seed)\n",
    "    \n",
    "    modified_parameter_sets = []\n",
    "    for params in parameter_sets:\n",
    "        new_params = {}\n",
    "        for key, value in params.items():\n",
    "            new_params[key] = value * rng.uniform(distort_range[0], distort_range[1])\n",
    "        modified_parameter_sets.append(new_params)\n",
    "    \n",
    "    return modified_parameter_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37a553",
   "metadata": {},
   "source": [
    "## Complete Bulk Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3466b749",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def complete_bulk_processing_pipeline(model_spec, solver, builder_models, exp_config):\n",
    "    \"\"\"\n",
    "    Complete pipeline including feature generation, distortion, and all data processing\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Starting complete bulk processing pipeline...\")\n",
    "    \n",
    "    # Extract parameters from builder models\n",
    "    parameter_sets = extract_parameter_sets_from_builders(builder_models)\n",
    "    \n",
    "    # Generate enhanced feature data\n",
    "    feature_data = generate_enhanced_feature_data(model_spec, builder_models, exp_config)\n",
    "    \n",
    "    # Apply parameter distortion if enabled\n",
    "    processed_parameter_sets = apply_parameter_distortion(parameter_sets, exp_config)\n",
    "    \n",
    "    # Simulation parameters\n",
    "    sim_config = exp_config['simulation']\n",
    "    sim_params = {'start': sim_config['start'], 'end': sim_config['stop'], 'points': sim_config['step']}\n",
    "    \n",
    "    # Generate target and timecourse data\n",
    "    outcome_var = exp_config['machine_learning']['outcome_var']\n",
    "    print(\"ðŸ“Š Generating target and timecourse data...\")\n",
    "    target_data, timecourse_data = generate_target_data_diff_build(\n",
    "        model_spec, solver, feature_data, processed_parameter_sets, sim_params,\n",
    "        outcome_var=outcome_var, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Generate enhanced timecourse data for dynamic features\n",
    "    print(\"ðŸ“ˆ Generating enhanced timecourse data for all species...\")\n",
    "    enhanced_timecourse = generate_model_timecourse_data_diff_build_v3(\n",
    "        builder_models[0].get_state_variables(), solver, feature_data, processed_parameter_sets, \n",
    "        sim_params, capture_species=\"all\", verbose=True\n",
    "    )\n",
    "    \n",
    "    # Calculate dynamic features with dynamic config exclusions\n",
    "    print(\"âš¡ Calculating dynamic features...\")\n",
    "    dynamic_config = exp_config['dynamic_data']\n",
    "    initial_values = builder_models[0].get_state_variables()\n",
    "    if dynamic_config.get('exclude_activated_form', False):\n",
    "        initial_values = {k: v for k, v in initial_values.items() if not k.endswith('a')}\n",
    "    if dynamic_config.get('excluded_layers'):\n",
    "        for layer in dynamic_config['excluded_layers']:\n",
    "            initial_values = {k: v for k, v in initial_values.items() if not k.startswith(layer)}\n",
    "    \n",
    "    dynamic_data = dynamic_features_method(enhanced_timecourse, initial_values.keys(), n_cores=4)\n",
    "    \n",
    "    # Calculate last time point data\n",
    "    print(\"â±ï¸ Calculating last time point data...\")\n",
    "    last_time_data = last_time_point_method(enhanced_timecourse, initial_values.keys())\n",
    "    \n",
    "    print(\"âœ… Complete bulk processing finished!\")\n",
    "    return feature_data, target_data, enhanced_timecourse, dynamic_data, last_time_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa66dc",
   "metadata": {},
   "source": [
    "## Run Bulk Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb77df49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting complete bulk processing pipeline...\n",
      "ðŸ“Š Generating target and timecourse data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating perturbations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:03<00:00, 273.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Generating enhanced timecourse data for all species...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating perturbations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:03<00:00, 251.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Calculating dynamic features...\n",
      "â±ï¸ Calculating last time point data...\n",
      "âœ… Complete bulk processing finished!\n"
     ]
    }
   ],
   "source": [
    "# Execute the complete bulk processing pipeline\n",
    "feature_data, target_data, timecourse_data, dynamic_data, last_time_data = complete_bulk_processing_pipeline(\n",
    "    new_spec, simulation_solver, builder_models, exp_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a52083",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7407ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_feature_data.pkl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72.7k/72.7k [00:00<00:00, 107kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… feature_data saved as pickle format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_feature_data.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88.8k/88.8k [00:00<00:00, 639kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… feature_data saved as parquet format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_target_data.pkl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.66k/8.66k [00:00<00:00, 89.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… target_data saved as pickle format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_target_data.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.4k/10.4k [00:00<00:00, 109kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… target_data saved as parquet format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_timecourse_data.pkl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.7M/16.7M [00:00<00:00, 25.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… timecourse_data saved as pickle format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_timecourse_data.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.8M/19.8M [00:00<00:00, 30.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… timecourse_data saved as parquet format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_dynamic_data.pkl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.60M/1.60M [00:00<00:00, 6.22MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dynamic_data saved as pickle format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_dynamic_data.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.15M/1.15M [00:00<00:00, 6.01MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dynamic_data saved as parquet format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_last_time_data.pkl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 161k/161k [00:00<00:00, 1.07MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… last_time_data saved as pickle format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading v1_last_time_data.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196k/196k [00:00<00:00, 1.37MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… last_time_data saved as parquet format\n",
      "ðŸ“ Final file list in S3:\n",
      "Config:\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/v1_config.yml\n",
      "Data:\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_dynamic_data.parquet\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_dynamic_data.pkl\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_feature_data.parquet\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_feature_data.pkl\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_last_time_data.parquet\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_last_time_data.pkl\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_sim_results.pkl\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_sim_results_df.parquet\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_sim_results_df.pkl\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_target_data.parquet\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_target_data.pkl\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_timecourse_data.parquet\n",
      "  - new-peak-project/experiments/ch5-paper/1_01_v1_diverse-synthetic-cohort-generation/data/v1_timecourse_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save all processed data types in both pickle and parquet formats\n",
    "data_types = {\n",
    "    'feature_data': feature_data,\n",
    "    'target_data': target_data, \n",
    "    'timecourse_data': timecourse_data,\n",
    "    'dynamic_data': dynamic_data,\n",
    "    'last_time_data': last_time_data\n",
    "}\n",
    "\n",
    "for data_name, data in data_types.items():\n",
    "    # Save as pickle format\n",
    "    s3_manager.save_data(\n",
    "        notebook_config=notebook_config,\n",
    "        data=data,\n",
    "        data_name=data_name,\n",
    "        data_format=\"pkl\",\n",
    "    )\n",
    "    print(f\"âœ… {data_name} saved as pickle format\")\n",
    "    \n",
    "    # Save as parquet format (if the data is a pandas DataFrame)\n",
    "    try:\n",
    "        if hasattr(data, 'to_parquet') or isinstance(data, (pd.DataFrame, pd.Series)):\n",
    "            s3_manager.save_data(\n",
    "                notebook_config=notebook_config,\n",
    "                data=data,\n",
    "                data_name=data_name,\n",
    "                data_format=\"parquet\",\n",
    "            )\n",
    "            print(f\"âœ… {data_name} saved as parquet format\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not save {data_name} as parquet: {e}\")\n",
    "\n",
    "# Verify all files are saved\n",
    "print(\"ðŸ“ Final file list in S3:\")\n",
    "files = s3_manager.list_experiment_files(notebook_config)\n",
    "for category, file_list in files.items():\n",
    "    if file_list:\n",
    "        print(f\"{category.capitalize()}:\")\n",
    "        for file in file_list:\n",
    "            print(f\"  - {file}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "new-peak-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
