"""
ML Batch Evaluation Runner - Configuration Version
For remote batch job execution

Loads CSV task lists generated by create-ml-loader-v1.py, runs batch ML evaluation,
and saves results to S3 under machine-learning/{data-eng-script}/{model}/ directory.

S3 Output Structure:
{save_result_path}/machine-learning/{data_eng_script}/{model_name}/
‚îú‚îÄ‚îÄ results.pkl            # Full evaluation results DataFrame
‚îú‚îÄ‚îÄ summary-stats.csv      # Aggregated statistics
‚îú‚îÄ‚îÄ run-metadata.yml       # Run configuration and metadata
‚îî‚îÄ‚îÄ failed-tasks.csv       # Tasks that failed during processing (if any)

CONFIGURATION INSTRUCTIONS:
1. Modify the configuration variables below
2. Run the script: python run-ml-batch-v1.py
3. For different runs, copy this script and modify the configuration
"""

import sys
import os
import pandas as pd
import numpy as np
import yaml
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
import traceback

# ===== CONFIGURATION =====
# Modify these variables for your batch job

# CSV file configuration
CSV_PATH = "path/to/your/tasks.csv"  # REQUIRED: Path to CSV task list generated by create-ml-loader-v1.py

# Experiment selection
EXPERIMENT_TYPES = ["expression-noise-v1"]  # List of experiment types to process
# EXPERIMENT_TYPES = ["expression-noise-v1", "parameter-distortion-v2"]  # Multiple experiments
# EXPERIMENT_TYPES = None  # Process ALL experiments in CSV

# Model configuration
MODEL_NAMES = None  # Auto-detect from CSV if None, otherwise specify: "sy_simple" or ["sy_simple", "v1"]
# Options:
# MODEL_NAMES = None  # Auto-detect from CSV (uses first model found)
# MODEL_NAMES = "sy_simple"  # Single model
# MODEL_NAMES = ["sy_simple", "v1", "fgfr4_model"]  # Multiple models

# Evaluation parameters
NUM_REPEATS = 10     # Number of random train/test splits
TEST_SIZE = 0.2      # Proportion of data for testing
RANDOM_SEED = 42     # Random seed for reproducibility
N_JOBS = -1          # Number of parallel jobs (-1 for all cores)

# Output configuration
UPLOAD_S3 = True     # Upload results to S3 (True/False)
SKIP_FAILED = True   # Skip failed tasks and continue processing

# ===== END CONFIGURATION =====

# Add src to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, "../../..")
sys.path.insert(0, src_dir)

from models.utils.s3_config_manager import S3ConfigManager
from src.notebooks.ch5_paper.data_eng.create_ml_loader_v1 import BatchLoader
from src.ml.Workflow import batch_eval_standard

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def validate_csv_structure(df: pd.DataFrame) -> bool:
    """Validate CSV has required columns"""
    required_columns = [
        "feature_data", "feature_data_label", 
        "target_data", "target_data_label",
        "experiment_type", "level", "model_name"
    ]
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logger.error(f"CSV missing required columns: {missing_columns}")
        return False
    
    return True


def normalize_model_names(model_names):
    """Convert model_names configuration to list format"""
    if model_names is None:
        return None
    elif isinstance(model_names, str):
        return [model_names]
    elif isinstance(model_names, list):
        return model_names
    else:
        logger.warning(f"Unexpected model_names type: {type(model_names)}, converting to list")
        return [str(model_names)]


def filter_tasks_by_model(df: pd.DataFrame, model_names: Optional[List[str]]) -> pd.DataFrame:
    """Filter tasks by model name(s)"""
    if model_names is None:
        return df
    
    filtered_df = df[df["model_name"].isin(model_names)].copy()
    logger.info(f"Filtered to {len(filtered_df)} tasks from {len(df)} total for models: {model_names}")
    
    return filtered_df


def filter_tasks_by_experiment(df: pd.DataFrame, experiment_types: Optional[List[str]]) -> pd.DataFrame:
    """Filter tasks by experiment type"""
    if experiment_types is None:
        return df
    
    filtered_df = df[df["experiment_type"].isin(experiment_types)].copy()
    logger.info(f"Filtered to {len(filtered_df)} tasks from {len(df)} total")
    logger.info(f"Experiment types: {experiment_types}")
    
    return filtered_df


def group_tasks_by_experiment(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """Group tasks by experiment type"""
    grouped = {}
    for exp_type, group_df in df.groupby("experiment_type"):
        grouped[exp_type] = group_df.reset_index(drop=True)
        logger.info(f"Experiment '{exp_type}': {len(group_df)} tasks")
    
    return grouped


def run_batch_evaluation_for_experiment(
    task_df: pd.DataFrame,
    experiment_type: str,
    model_name: str,
    s3_manager: S3ConfigManager,
    evaluation_params: Dict[str, Any]
) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Dict[str, Any]]:
    """
    Run batch evaluation for a specific experiment type
    
    Returns:
        Tuple of (results_df, failed_tasks_df, metadata)
    """
    logger.info(f"Starting batch evaluation for experiment: {experiment_type}")
    
    # Initialize BatchLoader
    loader = BatchLoader(s3_manager=s3_manager)
    
    # Create temporary CSV for this experiment
    temp_csv = f"temp_{experiment_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    task_df.to_csv(temp_csv, index=False)
    
    try:
        # Load task list
        loader.load_task_list(temp_csv)
        
        # Prepare data for batch_eval
        feature_data_list, feature_data_names, target_data, target_name = loader.prepare_for_batch_eval()
        
        logger.info(f"Prepared {len(feature_data_list)} feature datasets for evaluation")
        logger.info(f"Target column: {target_name}")
        
        # Run batch evaluation
        results_df = batch_eval_standard(
            feature_data_list=feature_data_list,
            feature_data_names=feature_data_names,
            target_data=target_data,
            target_name=target_name,
            num_repeats=evaluation_params["num_repeats"],
            test_size=evaluation_params["test_size"],
            o_random_seed=evaluation_params["random_seed"],
            n_jobs=evaluation_params["n_jobs"]
        )
        
        # Add experiment metadata to results
        results_df["experiment_type"] = experiment_type
        results_df["model_name"] = model_name
        results_df["evaluation_timestamp"] = datetime.now().isoformat()
        
        logger.info(f"Batch evaluation completed: {len(results_df)} results generated")
        
        # Generate metadata
        metadata = {
            "experiment_type": experiment_type,
            "model_name": model_name,
            "evaluation_params": evaluation_params,
            "task_stats": {
                "total_tasks": len(task_df),
                "successful_tasks": len(task_df),  # All succeeded if we got here
                "failed_tasks": 0
            },
            "results_stats": {
                "total_results": len(results_df),
                "unique_models": results_df["Model"].nunique(),
                "unique_features": results_df["Feature Data"].nunique(),
                "mean_r2": results_df["R2 Score"].mean(),
                "mean_mse": results_df["Mean Squared Error"].mean()
            },
            "timestamp": datetime.now().isoformat(),
            "script_version": "run-ml-batch-v1.py"
        }
        
        failed_tasks_df = None
        
    except Exception as e:
        logger.error(f"Batch evaluation failed for experiment {experiment_type}: {e}")
        logger.error(traceback.format_exc())
        
        results_df = None
        metadata = {
            "experiment_type": experiment_type,
            "model_name": model_name,
            "error": str(e),
            "error_traceback": traceback.format_exc(),
            "timestamp": datetime.now().isoformat(),
            "status": "failed"
        }
        
        # Create failed tasks DataFrame
        failed_tasks_df = task_df.copy()
        failed_tasks_df["error"] = str(e)
        failed_tasks_df["failed_timestamp"] = datetime.now().isoformat()
        
    finally:
        # Clean up temporary file
        if os.path.exists(temp_csv):
            os.remove(temp_csv)
    
    return results_df, failed_tasks_df, metadata


def generate_summary_stats(results_df: pd.DataFrame) -> pd.DataFrame:
    """Generate summary statistics from results"""
    if results_df is None or len(results_df) == 0:
        return pd.DataFrame()
    
    # Group by Model and Feature Data
    summary = results_df.groupby(["Model", "Feature Data"]).agg({
        "R2 Score": ["mean", "std", "min", "max"],
        "Mean Squared Error": ["mean", "std", "min", "max"],
        "Pearson Correlation": ["mean", "std", "min", "max"]
    }).round(4)
    
    # Flatten column names
    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]
    summary = summary.reset_index()
    
    return summary


def save_results_to_s3(
    results_df: pd.DataFrame,
    summary_df: pd.DataFrame,
    metadata: Dict[str, Any],
    failed_tasks_df: Optional[pd.DataFrame],
    s3_manager: S3ConfigManager,
    experiment_type: str,
    model_name: str
) -> Dict[str, str]:
    """
    Save results to S3 with structure: machine-learning/{experiment_type}/{model_name}/
    
    Returns dictionary of saved S3 paths
    """
    # Construct S3 paths
    base_path = f"{s3_manager.save_result_path}/machine-learning/{experiment_type}/{model_name}"
    
    s3_paths = {}
    
    # Save results DataFrame
    if results_df is not None and len(results_df) > 0:
        results_path = f"{base_path}/results.pkl"
        s3_manager.save_data_from_path(results_path, results_df, data_format="pkl")
        s3_paths["results"] = results_path
        logger.info(f"‚úÖ Saved results to S3: {results_path}")
    
    # Save summary statistics
    if summary_df is not None and len(summary_df) > 0:
        summary_path = f"{base_path}/summary-stats.csv"
        s3_manager.save_data_from_path(summary_path, summary_df, data_format="csv")
        s3_paths["summary"] = summary_path
        logger.info(f"‚úÖ Saved summary stats to S3: {summary_path}")
    
    # Save metadata
    metadata_path = f"{base_path}/run-metadata.yml"
    s3_manager.save_data_from_path(metadata_path, metadata, data_format="txt")
    s3_paths["metadata"] = metadata_path
    logger.info(f"‚úÖ Saved metadata to S3: {metadata_path}")
    
    # Save failed tasks (if any)
    if failed_tasks_df is not None and len(failed_tasks_df) > 0:
        failed_path = f"{base_path}/failed-tasks.csv"
        s3_manager.save_data_from_path(failed_path, failed_tasks_df, data_format="csv")
        s3_paths["failed_tasks"] = failed_path
        logger.info(f"‚úÖ Saved failed tasks to S3: {failed_path}")
    
    return s3_paths




def main():
    """Main execution function using configuration variables with multi-model support"""
    # Use configuration variables
    csv_path = CSV_PATH
    experiment_types = EXPERIMENT_TYPES
    model_names = MODEL_NAMES
    num_repeats = NUM_REPEATS
    test_size = TEST_SIZE
    random_seed = RANDOM_SEED
    n_jobs = N_JOBS
    upload_s3 = UPLOAD_S3
    skip_failed = SKIP_FAILED
    
    # Normalize model names to list format
    model_names_normalized = normalize_model_names(model_names)
    
    logger.info("üöÄ Starting ML Batch Evaluation Runner (Configuration Version)")
    logger.info(f"CSV: {csv_path}")
    logger.info(f"Experiment types: {experiment_types}")
    logger.info(f"Upload to S3: {upload_s3}")
    if not upload_s3:
        logger.warning("‚ö†Ô∏è  WARNING: UPLOAD_S3 is False - results will not be saved to S3 or locally")
    logger.info(f"Model names: {model_names_normalized or 'Auto-detect from CSV'}")
    logger.info(f"Num repeats: {num_repeats}, Test size: {test_size}")
    logger.info(f"Random seed: {random_seed}, Parallel jobs: {n_jobs}")
    
    start_time = datetime.now()
    overall_results = {}
    
    try:
        # Load and validate CSV
        if not os.path.exists(csv_path):
            logger.error(f"CSV file not found: {csv_path}")
            sys.exit(1)
        
        task_df = pd.read_csv(csv_path)
        logger.info(f"Loaded CSV with {len(task_df)} tasks")
        
        if not validate_csv_structure(task_df):
            sys.exit(1)
        
        # Determine models to process
        if model_names_normalized is None:
            # Auto-detect: use all unique models in CSV
            model_names_normalized = task_df["model_name"].unique().tolist()
            logger.info(f"Auto-detected models: {model_names_normalized}")
        
        # Initialize S3 manager if needed
        s3_manager = None
        if upload_s3:
            s3_manager = S3ConfigManager()
            logger.info("‚úÖ S3 manager initialized")
        
        # Evaluation parameters
        evaluation_params = {
            "num_repeats": num_repeats,
            "test_size": test_size,
            "random_seed": random_seed,
            "n_jobs": n_jobs
        }
        
        # Process each model
        for model_name in model_names_normalized:
            logger.info(f"\n{'='*60}")
            logger.info(f"PROCESSING MODEL: {model_name}")
            logger.info(f"{'='*60}")
            
            # Filter tasks for this model
            model_tasks = filter_tasks_by_model(task_df, [model_name])
            if len(model_tasks) == 0:
                logger.warning(f"No tasks found for model {model_name}, skipping")
                continue
            
            # Filter by experiment types
            filtered_df = filter_tasks_by_experiment(model_tasks, experiment_types)
            if len(filtered_df) == 0:
                logger.warning(f"No tasks match experiment criteria for model {model_name}, skipping")
                continue
            
            # Group by experiment type
            grouped_tasks = group_tasks_by_experiment(filtered_df)
            
            # Process each experiment type for this model
            model_results = {}
            model_failed_tasks = []
            
            for experiment_type, exp_task_df in grouped_tasks.items():
                logger.info(f"\n  Processing experiment: {experiment_type}")
                logger.info(f"  {'-'*40}")
                
                # Run batch evaluation
                results_df, failed_tasks_df, metadata = run_batch_evaluation_for_experiment(
                    task_df=exp_task_df,
                    experiment_type=experiment_type,
                    model_name=model_name,
                    s3_manager=s3_manager,
                    evaluation_params=evaluation_params
                )
                
                # Generate summary statistics
                summary_df = None
                if results_df is not None:
                    summary_df = generate_summary_stats(results_df)
                
                # Save results
                if upload_s3 and s3_manager is not None:
                    s3_paths = save_results_to_s3(
                        results_df=results_df,
                        summary_df=summary_df,
                        metadata=metadata,
                        failed_tasks_df=failed_tasks_df,
                        s3_manager=s3_manager,
                        experiment_type=experiment_type,
                        model_name=model_name
                    )
                    metadata["s3_paths"] = s3_paths
                
                # Skip local saving (OUTPUT_DIR removed per user request)
                # Results are only saved to S3 when UPLOAD_S3 = True
                
                # Store results
                model_results[experiment_type] = {
                    "results": results_df,
                    "summary": summary_df,
                    "metadata": metadata
                }
                
                if failed_tasks_df is not None:
                    model_failed_tasks.append(failed_tasks_df)
                
                logger.info(f"  ‚úÖ Completed experiment: {experiment_type}")
            
            # Store model results
            overall_results[model_name] = {
                "experiment_results": model_results,
                "failed_tasks": model_failed_tasks,
                "total_experiments": len(model_results)
            }
            
            logger.info(f"‚úÖ Completed processing for model: {model_name}")
        
        # Calculate execution time
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Print summary
        logger.info(f"\n{'='*60}")
        logger.info("üìä EXECUTION SUMMARY")
        logger.info(f"{'='*60}")
        logger.info(f"Total models processed: {len(overall_results)}")
        logger.info(f"Total execution time: {duration:.2f} seconds ({duration/60:.2f} minutes)")
        
        successful_models = []
        failed_models = []
        
        for model_name, model_data in overall_results.items():
            successful_exps = [exp for exp, data in model_data["experiment_results"].items() if data["results"] is not None]
            failed_exps = [exp for exp, data in model_data["experiment_results"].items() if data["results"] is None]
            
            if failed_exps:
                failed_models.append(model_name)
            else:
                successful_models.append(model_name)
            
            logger.info(f"\n  Model: {model_name}")
            logger.info(f"    Experiments: {len(model_data['experiment_results'])} total")
            logger.info(f"    Successful: {len(successful_exps)}")
            if failed_exps:
                logger.info(f"    Failed: {len(failed_exps)} - {', '.join(failed_exps)}")
        
        logger.info(f"\n‚úÖ Successful models: {len(successful_models)}")
        if successful_models:
            logger.info(f"  - {', '.join(successful_models)}")
        
        if failed_models:
            logger.info(f"‚ùå Models with failures: {len(failed_models)}")
            logger.info(f"  - {', '.join(failed_models)}")
        
        # Skip overall summary saving (OUTPUT_DIR removed per user request)
        
        logger.info("üéâ ML Batch Evaluation completed successfully!")
        
    except Exception as e:
        logger.error(f"‚ùå ML Batch Evaluation failed: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()
