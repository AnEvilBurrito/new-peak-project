{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems perturbation of links in the biochemical cascade network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "# find the string 'project' in the path, return index\n",
    "index_project = path.find('project')\n",
    "# slice the path from the index of 'project' to the end\n",
    "project_path = path[:index_project+7]\n",
    "# set the working directory\n",
    "os.chdir(project_path+'\\src')\n",
    "print(f'Project path set to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "print(config[\"DATA_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ModelBuilder import ModelBuilder\n",
    "from models.Reaction import Reaction\n",
    "from models.ReactionArchtype import ReactionArchtype\n",
    "from models.ArchtypeCollections import *\n",
    "from models.Utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import roadrunner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# tree models and support vector machines\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import pearson correlation\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "### parameters \n",
    "\n",
    "notebook_name = 'systems_perturbation_dyn_feats'\n",
    "plot_figures = True\n",
    "run_jobs = True\n",
    "parallelise = True\n",
    "save_figures = True\n",
    "\n",
    "## Generation of ground truth model \n",
    "\n",
    "model_name = 'v3_small_model_52'\n",
    "o_random_seed = 4\n",
    "no_observable_species = 5\n",
    "no_feedback_regulations = 2\n",
    "specie_value_range = (5, 5000)\n",
    "param_range = (0.1, 10)\n",
    "param_multiplier_range = (0.7, 1.3)\n",
    "\n",
    "\n",
    "## Simulation parameters \n",
    "\n",
    "simulation_time = 500 \n",
    "simulation_step = 100\n",
    "\n",
    "## Feature data generation \n",
    "\n",
    "feature_generation_method = 'uniform'\n",
    "feature_generation_extra_params = {'min': 0.1, 'max': 10}\n",
    "feature_generation_size = 1000 \n",
    "feature_generation_seed = 50 # if -1 then 'o_random_seed' is used\n",
    "if feature_generation_seed == -1:\n",
    "    feature_generation_seed = o_random_seed\n",
    "''' \n",
    "Options: \n",
    "- 'last_time_point' : only the last time point of the phosphorylated species is used\n",
    "- 'dynamic_feature': computes the characteristic 'ten' dynamic feature for each specie data \n",
    "'''\n",
    "\n",
    "## General parameters \n",
    "experiment_id = notebook_name + '_' + str(o_random_seed) + '_' + str(feature_generation_seed)\n",
    "experiment_folder = config['DATA_PATH'] + '/' + experiment_id + '/'\n",
    "if not os.path.exists(experiment_folder):\n",
    "    os.makedirs(experiment_folder)\n",
    "    \n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ground truth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the ground truth model\n",
    "\n",
    "model_spec = ModelSpecification()\n",
    "model_spec.generate_specifications(o_random_seed, no_observable_species, no_feedback_regulations, verbose=0)\n",
    "G0 = model_spec.generate_network(model_name, specie_value_range, param_range, param_multiplier_range, random_seed=o_random_seed, verbose=0)\n",
    "runner = roadrunner.RoadRunner(G0.get_sbml_model())\n",
    "res = runner.simulate(0, simulation_time, simulation_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOT: Visible States Over Time ===\n",
    "\n",
    "if plot_figures:\n",
    "\n",
    "    import seaborn as sns\n",
    "    sns.set_context('talk')\n",
    "    sns.axes_style('whitegrid')\n",
    "\n",
    "    # Initialize RoadRunner and simulate the model\n",
    "    runner = roadrunner.RoadRunner(G0.get_sbml_model())\n",
    "    res = runner.simulate(0, simulation_time, simulation_step)\n",
    "\n",
    "    # Define key observable states\n",
    "    visible_states = ['C', 'Cp']\n",
    "\n",
    "    # Plot the concentration of visible states over time\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for state in visible_states:\n",
    "        plt.plot(res['time'], res[f'[{state}]'], label=state)\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Concentration\")\n",
    "    plt.title(\"Concentration of C and Cp over Time\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_figures:\n",
    "        plt.savefig(experiment_folder + 'Fig_01_C_Cp_time_course_ground_truth.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the synthetic data\n",
    "\n",
    "feature_df = generate_feature_data(model_spec, runner, feature_generation_method, feature_generation_extra_params, feature_generation_size, seed=feature_generation_seed)\n",
    "initial_values = get_model_initial_values(model_spec, runner)\n",
    "\n",
    "target_df, time_course_data = generate_target_data(model_spec, runner, feature_df, initial_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate suboptimal models (n edge removed random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sub = 10\n",
    "suboptimal_models_rms = {}\n",
    "suboptimal_models_spec_rms = {}\n",
    "\n",
    "n_edge_removed_range = (2,9)\n",
    "\n",
    "for n_edge_removed in range(n_edge_removed_range[0], n_edge_removed_range[1]+1):\n",
    "    suboptimal_models_rms[n_edge_removed] = []\n",
    "    suboptimal_models_spec_rms[n_edge_removed] = []\n",
    "    for i in range(n_sub):\n",
    "        s_spec, s_model = systematic_edge_pruning(model_spec, G0, n_edge_removed, random_seed=i)\n",
    "        suboptimal_models_spec_rms[n_edge_removed].append(s_spec)\n",
    "        suboptimal_models_rms[n_edge_removed].append(s_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_figures:\n",
    "    # Visualise the behaviour of the suboptimal models, plot the original model as reference comparison\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    for i, s_model in enumerate(suboptimal_models_rms[5]):\n",
    "        runner = roadrunner.RoadRunner(s_model.get_sbml_model())\n",
    "        res = runner.simulate(0, simulation_time, simulation_step)\n",
    "        plt.plot(res['time'], res['[Cp]'], label=f\"Model {i+1}\")\n",
    "        \n",
    "    runner = roadrunner.RoadRunner(G0.get_sbml_model())\n",
    "    res = runner.simulate(0, simulation_time, simulation_step)\n",
    "    plt.plot(res['time'], res['[Cp]'], label=\"Ground Truth\", linestyle='dashed', color='black')\n",
    "        \n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Concentration\")\n",
    "    plt.title(\"Concentration of Cp over Time for Suboptimal Models\")\n",
    "    plt.grid(True)\n",
    "    # set the legend to be outside the plot\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate timecourse data from ground truth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the last time point for each phosphorylated species\n",
    "all_species = model_spec.A_species + model_spec.B_species + model_spec.C_species\n",
    "all_phos_species = [s+'p' for s in all_species]\n",
    "all_time_course_data_G0 = generate_model_timecourse_data(model_spec, runner, feature_df, initial_values)\n",
    "all_time_course_data_phos_G0 = all_time_course_data_G0[all_phos_species]\n",
    "dynamic_feature_df_G0 = dynamic_features_method(all_time_course_data_G0, all_phos_species)\n",
    "\n",
    "\n",
    "if run_jobs:\n",
    "    time_course_data_rm_all = {}\n",
    "    all_species = model_spec.A_species + model_spec.B_species + model_spec.C_species\n",
    "    all_phos_species = [s+'p' for s in all_species]\n",
    "\n",
    "    for n_edge_removed in range(n_edge_removed_range[0], n_edge_removed_range[1]+1):\n",
    "        time_course_data_rm = []\n",
    "        for i in range(n_sub):\n",
    "            s_spec = suboptimal_models_spec_rms[n_edge_removed][i]\n",
    "            s_model = suboptimal_models_rms[n_edge_removed][i]\n",
    "            runner = roadrunner.RoadRunner(s_model.get_sbml_model())\n",
    "            time_course_data = generate_model_timecourse_data(s_spec, runner, feature_df, initial_values)\n",
    "            time_course_data_rm.append(time_course_data)\n",
    "        time_course_data_rm_all[n_edge_removed] = time_course_data_rm\n",
    "        \n",
    "\n",
    "if run_jobs:\n",
    "    dynamic_features_rm_all = {}\n",
    "    if not parallelise: \n",
    "        for n_edge_removed in range(n_edge_removed_range[0], n_edge_removed_range[1]+1):\n",
    "            dynamic_features_rm = []\n",
    "            for i in range(n_sub):\n",
    "                time_course_data = time_course_data_rm_all[n_edge_removed][i]\n",
    "                dynamic_features = dynamic_features_method(time_course_data, all_phos_species)\n",
    "                dynamic_features_rm.append(dynamic_features)\n",
    "            dynamic_features_rm_all[n_edge_removed] = dynamic_features_rm\n",
    "    else: \n",
    "        from joblib import Parallel, delayed\n",
    "        for n_edge_removed in range(n_edge_removed_range[0], n_edge_removed_range[1]+1):\n",
    "            dynamic_features_rm = Parallel(n_jobs=-1)(delayed(dynamic_features_method)(time_course_data, all_phos_species) for time_course_data in time_course_data_rm_all[n_edge_removed])\n",
    "            dynamic_features_rm_all[n_edge_removed] = dynamic_features_rm\n",
    "            print(f\"Completed n_edge_removed={n_edge_removed}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning on G0 last time point features and all 2rm suboptimal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, feature_data, feature_data_name, target_data ,test_size=0.2, random_state=4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=test_size, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # return a dictionary of the model performance\n",
    "    return {'Model': model_name, \n",
    "            'Feature Data': feature_data_name,\n",
    "            'Mean Squared Error': mean_squared_error(y_test, y_pred),\n",
    "            'R2 Score': r2_score(y_test, y_pred),\n",
    "            'Pearson Correlation': pearsonr(y_test, y_pred)[0],\n",
    "            'Pearson P-Value': pearsonr(y_test, y_pred)[1]\n",
    "            }\n",
    "# create a linear regression model\n",
    "lm = LinearRegression()\n",
    "# create a random forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=o_random_seed)\n",
    "# create a gradient boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, random_state=o_random_seed)\n",
    "# create a support vector machine model\n",
    "svr = SVR(max_iter=10000)\n",
    "scaled_svr = Pipeline([('scaler', StandardScaler()), ('svr', svr)])\n",
    "# create a neural network model (simple)\n",
    "nn = MLPRegressor(hidden_layer_sizes=(20,), max_iter=10000, random_state=o_random_seed)\n",
    "scaled_nn = Pipeline([('scaler', StandardScaler()), ('nn', nn)])\n",
    "\n",
    "\n",
    "all_metric_dfs = {}\n",
    "\n",
    "for n_edge_removed in range(n_edge_removed_range[0], n_edge_removed_range[1]+1):\n",
    "    dynamic_features = dynamic_features_rm_all[n_edge_removed]\n",
    "\n",
    "\n",
    "    all_models = [lm, rf, gb, scaled_svr, scaled_nn]\n",
    "    all_models_desc = ['Linear Regression', 'Random Forest', 'Gradient Boosting', 'Support Vector Machine', 'Neural Network']\n",
    "    zipped_model_data = list(zip(all_models, all_models_desc))\n",
    "    all_features = [feature_df, dynamic_feature_df_G0] + dynamic_features\n",
    "    all_features_desc = ['Mock Omics Data (A+B)', 'Last Point G0'] + [f'Last Point S{i+1} {n_edge_removed}rm' for i in range(n_sub)]\n",
    "    zipped_feature_data = list(zip(all_features, all_features_desc))\n",
    "\n",
    "    # random states are rand ints between 0 and 10000, for n values \n",
    "    np.random.seed(o_random_seed)\n",
    "    n_random = 10\n",
    "    all_random_states = np.random.randint(0, 10000, n_random)\n",
    "\n",
    "    if run_jobs: \n",
    "        metric_data = []\n",
    "        if not parallelise:          \n",
    "            for (feature_data, feature_data_name) in zipped_feature_data:\n",
    "                # print('Feature Data:', feature_data_name)\n",
    "                # print('Feature Data Shape:', feature_data.shape)\n",
    "                for (model, model_name) in zipped_model_data:\n",
    "                    # print('Model:', model_name)\n",
    "                    for rand in all_random_states:\n",
    "                        metrics = evaluate_model(model, model_name, feature_data, feature_data_name, target_df['Cp'], random_state=rand)\n",
    "                        metric_data.append(metrics)\n",
    "                        \n",
    "        else:        \n",
    "            # parallelise the model evaluation process using joblib\n",
    "            from joblib import Parallel, delayed\n",
    "\n",
    "            metric_data = Parallel(n_jobs=-1)(delayed(evaluate_model)(model, model_name, feature_data, feature_data_name, target_df['Cp'], random_state=rand) \n",
    "                                            for (feature_data, feature_data_name) in zipped_feature_data\n",
    "                                            for (model, model_name) in zipped_model_data\n",
    "                                            for rand in all_random_states)\n",
    "\n",
    "        # make a dataframe of the metric data\n",
    "        metric_df = pd.DataFrame(metric_data)\n",
    "        # add a column for the number of edges removed\n",
    "        metric_df['Edges Removed'] = n_edge_removed\n",
    "        # add the dataframe to the dictionary\n",
    "        all_metric_dfs[n_edge_removed] = metric_df\n",
    "        print('--- Finished evaluating models for', n_edge_removed, 'edges removed ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metric_df \n",
    "if save_figures and run_jobs: \n",
    "    for n_edge_removed in range(n_edge_removed_range[0], n_edge_removed_range[1]+1):\n",
    "        all_metric_dfs[n_edge_removed].to_pickle(experiment_folder + f'{n_edge_removed}rm_metric_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_figures:\n",
    "\n",
    "    metric_df_loaded = '9rm_metric_df'\n",
    "    # load metric_df from data_path\n",
    "    metric_df = pd.read_pickle(experiment_folder+f'{metric_df_loaded}.pkl')\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(22,10)})\n",
    "    # make the font size bigger for labels and title, and axes labels \n",
    "    sns.set(font_scale=1.8)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.stripplot(data=metric_df, x='Model', y='Pearson Correlation', hue='Feature Data', dodge=True, jitter=True, size=10, alpha=0.8, legend=False)\n",
    "\n",
    "    # display the mean value for each model and feature data type\n",
    "    mean_vals = metric_df.groupby(['Model', 'Feature Data']).mean().reset_index()\n",
    "        \n",
    "\n",
    "    # make a transparent box plot, without using the alpha parameter\n",
    "    sns.boxplot(data=metric_df, x='Model', y='Pearson Correlation', hue='Feature Data')\n",
    "    for patch in plt.gca().patches:\n",
    "        patch.set_alpha(0.3)\n",
    "\n",
    "    plt.title('Model Performance Comparison for Pearson Correlation')\n",
    "\n",
    "    if save_figures:\n",
    "        plt.savefig(experiment_folder + f'Fig_{metric_df_loaded}_model_performance_all.png')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-peak-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
