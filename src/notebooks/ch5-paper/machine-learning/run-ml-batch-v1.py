"""
ML Batch Evaluation Runner - Configuration Version
For remote batch job execution

Loads CSV task lists generated by create-ml-loader-v1.py, runs batch ML evaluation,
and saves results to S3 under machine-learning/{data-eng-script}/{model}/ directory.

S3 Output Structure:
{save_result_path}/machine-learning/{data_eng_script}/{model_name}/
‚îú‚îÄ‚îÄ results.pkl            # Full evaluation results DataFrame
‚îú‚îÄ‚îÄ summary-stats.csv      # Aggregated statistics
‚îú‚îÄ‚îÄ run-metadata.yml       # Run configuration and metadata
‚îî‚îÄ‚îÄ failed-tasks.csv       # Tasks that failed during processing (if any)

CONFIGURATION INSTRUCTIONS:
1. Modify the configuration variables below
2. Run the script: python run-ml-batch-v1.py
3. For different runs, copy this script and modify the configuration
"""

import sys
import os
import pandas as pd
import numpy as np
import yaml
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
import traceback

# ===== CONFIGURATION =====
# Modify these variables for your batch job

# CSV file configuration
CSV_PATH = None  # Set to None for auto-discovery, or provide custom path: "path/to/your/tasks.csv"
# When CSV_PATH is None, the script will auto-discover task lists based on EXPERIMENT_TYPES and MODEL_NAMES
# When CSV_PATH is provided, the script will load the specified CSV file

# Experiment selection
EXPERIMENT_TYPES = ["response-noise-v1"]  # List of experiment types to process
# EXPERIMENT_TYPES = ["expression-noise-v1", "parameter-distortion-v2"]  # Multiple experiments
# EXPERIMENT_TYPES = None  # Process ALL experiments in CSV

# Model configuration
MODEL_NAMES = ["sy_simple"]  # List of model names to process
# Options:
# MODEL_NAMES = ["sy_simple"]  # Single model
# MODEL_NAMES = ["sy_simple", "v1", "fgfr4_model"]  # Multiple models
# IMPORTANT: For auto-discovery (CSV_PATH=None), MODEL_NAMES must be specified as a list

# Evaluation parameters
NUM_REPEATS = 5     # Number of random train/test splits
TEST_SIZE = 0.2      # Proportion of data for testing
RANDOM_SEED = 42     # Random seed for reproducibility
N_JOBS = -1          # Number of parallel jobs (-1 for all cores)

# Output configuration
UPLOAD_S3 = True     # Upload results to S3 (True/False)
SKIP_FAILED = True   # Skip failed tasks and continue processing

# Notification configuration (matching data-eng scripts)
SEND_NOTIFICATIONS = True   # Send ntfy notifications (True/False)

# ===== END CONFIGURATION =====

# Add src to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, "../../..")
sys.path.insert(0, src_dir)

from models.utils.s3_config_manager import S3ConfigManager
from scripts.ntfy_notifier import notify_start, notify_success, notify_failure

# Import BatchLoader from the correct location - note the hyphen in the directory name
# The file is at: src/notebooks/ch5-paper/data-eng/create-ml-loader-v1.py
# We need to handle the hyphens properly
import importlib.util

# Helper to import from hyphenated module
def import_from_hyphenated_file(filepath, class_name):
    """Import a class from a Python file with hyphenated name"""
    module_name = filepath.replace('-', '_').replace('.py', '')
    file_dir = os.path.dirname(filepath)
    
    # Add the file's directory to sys.path so it can find local imports
    original_sys_path = sys.path.copy()
    if file_dir not in sys.path:
        sys.path.insert(0, file_dir)
    
    try:
        spec = importlib.util.spec_from_file_location(module_name, filepath)
        module = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = module
        spec.loader.exec_module(module)
        return getattr(module, class_name)
    finally:
        # Restore original sys.path
        sys.path = original_sys_path

# Import BatchLoader from the create-ml-loader-v1.py file
create_loader_path = os.path.join(current_dir, "../data-eng/create-ml-loader-v1.py")
BatchLoader = import_from_hyphenated_file(create_loader_path, "BatchLoader")

# Import batch_eval_standard from ml.Workflow
# The ml directory is under src/ml/Workflow.py
ml_dir = os.path.join(src_dir, "ml")
sys.path.insert(0, ml_dir)

# Try to import
try:
    from Workflow import batch_eval_standard
except ImportError:
    # Try alternative import path
    sys.path.insert(0, os.path.join(src_dir, "ml"))
    import Workflow
    batch_eval_standard = Workflow.batch_eval_standard

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def discover_task_lists(experiment_types, model_names, s3_manager):
    """
    Discover task list CSVs based on EXPERIMENT_TYPES and MODEL_NAMES
    
    Looks for CSVs at: {s3_path}/data/{model_name}_{experiment_type}/task_list.csv
    Returns: Combined DataFrame of all discovered tasks
    """
    all_tasks = []
    
    for model_name in model_names:
        for exp_type in experiment_types:
            # Construct expected CSV path based on pattern from data-eng scripts
            # Data-eng scripts create folders with underscores: {model_name}_{experiment_type_with_underscores}
            # e.g., "sy_simple_expression_noise_v1" (underscores, not hyphens)
            # So we need to convert hyphens to underscores for folder names
            folder_name = f"{model_name}_{exp_type.replace('-', '_')}"
            csv_path = f"{s3_manager.save_result_path}/data/{folder_name}/task_list.csv"
            
            try:
                logger.info(f"Looking for task list: {csv_path}")
                task_df = s3_manager.load_data_from_path(csv_path, data_format="csv")
                
                # The CSV already contains tasks for this specific experiment type (folder name matches)
                # No need to filter by experiment_type column - all rows are valid
                if not task_df.empty:
                    all_tasks.append(task_df)
                    logger.info(f"Found {len(task_df)} tasks for {model_name}/{exp_type}")
                else:
                    logger.warning(f"CSV is empty for {model_name}/{exp_type}")
                    
            except Exception as e:
                logger.warning(f"Task list not found or error loading: {csv_path} - {str(e)[:100]}")
    
    if all_tasks:
        combined_df = pd.concat(all_tasks, ignore_index=True)
        logger.info(f"Discovered total of {len(combined_df)} tasks across all experiments/models")
        return combined_df
    else:
        logger.error("No task lists discovered for the specified experiments/models")
        return pd.DataFrame()


def validate_csv_structure(df: pd.DataFrame) -> bool:
    """Validate CSV has required columns"""
    required_columns = [
        "feature_data", "feature_data_label", 
        "target_data", "target_data_label",
        "experiment_type", "level", "model_name"
    ]
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logger.error(f"CSV missing required columns: {missing_columns}")
        return False
    
    return True


def normalize_model_names(model_names):
    """Convert model_names configuration to list format"""
    if model_names is None:
        return None
    elif isinstance(model_names, str):
        return [model_names]
    elif isinstance(model_names, list):
        return model_names
    else:
        logger.warning(f"Unexpected model_names type: {type(model_names)}, converting to list")
        return [str(model_names)]


def filter_tasks_by_model(df: pd.DataFrame, model_names: Optional[List[str]]) -> pd.DataFrame:
    """Filter tasks by model name(s)"""
    if model_names is None:
        return df
    
    filtered_df = df[df["model_name"].isin(model_names)].copy()
    logger.info(f"Filtered to {len(filtered_df)} tasks from {len(df)} total for models: {model_names}")
    
    return filtered_df




def run_batch_evaluation_for_experiment(
    task_df: pd.DataFrame,
    experiment_type: str,
    model_name: str,
    s3_manager: S3ConfigManager,
    evaluation_params: Dict[str, Any]
) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Dict[str, Any]]:
    """
    Run batch evaluation for a specific experiment type
    
    Returns:
        Tuple of (results_df, failed_tasks_df, metadata)
    """
    logger.info(f"Starting batch evaluation for experiment: {experiment_type}")
    
    # Initialize BatchLoader
    loader = BatchLoader(s3_manager=s3_manager)
    
    # Create temporary CSV for this experiment
    temp_csv = f"temp_{experiment_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    task_df.to_csv(temp_csv, index=False)
    
    try:
        # Load task list
        loader.load_task_list(temp_csv)
        
        # Prepare data for batch_eval
        feature_data_list, feature_data_names, target_data, target_name = loader.prepare_for_batch_eval()
        
        logger.info(f"Prepared {len(feature_data_list)} feature datasets for evaluation")
        logger.info(f"Target column: {target_name}")
        
        # Run batch evaluation
        results_df = batch_eval_standard(
            feature_data_list=feature_data_list,
            feature_data_names=feature_data_names,
            target_data=target_data,
            target_name=target_name,
            num_repeats=evaluation_params["num_repeats"],
            test_size=evaluation_params["test_size"],
            o_random_seed=evaluation_params["random_seed"],
            n_jobs=evaluation_params["n_jobs"]
        )
        
        # Add experiment metadata to results
        results_df["experiment_type"] = experiment_type
        results_df["model_name"] = model_name
        results_df["evaluation_timestamp"] = datetime.now().isoformat()
        
        logger.info(f"Batch evaluation completed: {len(results_df)} results generated")
        
        # Generate metadata
        metadata = {
            "experiment_type": experiment_type,
            "model_name": model_name,
            "evaluation_params": evaluation_params,
            "task_stats": {
                "total_tasks": len(task_df),
                "successful_tasks": len(task_df),  # All succeeded if we got here
                "failed_tasks": 0
            },
            "results_stats": {
                "total_results": len(results_df),
                "unique_models": results_df["Model"].nunique(),
                "unique_features": results_df["Feature Data"].nunique(),
                "mean_r2": results_df["R2 Score"].mean(),
                "mean_mse": results_df["Mean Squared Error"].mean()
            },
            "timestamp": datetime.now().isoformat(),
            "script_version": "run-ml-batch-v1.py"
        }
        
        failed_tasks_df = None
        
    except Exception as e:
        logger.error(f"Batch evaluation failed for experiment {experiment_type}: {e}")
        logger.error(traceback.format_exc())
        
        results_df = None
        metadata = {
            "experiment_type": experiment_type,
            "model_name": model_name,
            "error": str(e),
            "error_traceback": traceback.format_exc(),
            "timestamp": datetime.now().isoformat(),
            "status": "failed"
        }
        
        # Create failed tasks DataFrame
        failed_tasks_df = task_df.copy()
        failed_tasks_df["error"] = str(e)
        failed_tasks_df["failed_timestamp"] = datetime.now().isoformat()
        
    finally:
        # Clean up temporary file
        if os.path.exists(temp_csv):
            os.remove(temp_csv)
    
    return results_df, failed_tasks_df, metadata


def generate_summary_stats(results_df: pd.DataFrame) -> pd.DataFrame:
    """Generate summary statistics from results"""
    if results_df is None or len(results_df) == 0:
        return pd.DataFrame()
    
    # Group by Model and Feature Data
    summary = results_df.groupby(["Model", "Feature Data"]).agg({
        "R2 Score": ["mean", "std", "min", "max"],
        "Mean Squared Error": ["mean", "std", "min", "max"],
        "Pearson Correlation": ["mean", "std", "min", "max"]
    }).round(4)
    
    # Flatten column names
    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]
    summary = summary.reset_index()
    
    return summary


def save_results_to_s3(
    results_df: pd.DataFrame,
    summary_df: pd.DataFrame,
    metadata: Dict[str, Any],
    failed_tasks_df: Optional[pd.DataFrame],
    s3_manager: S3ConfigManager,
    experiment_type: str,
    model_name: str
) -> Dict[str, str]:
    """
    Save results to S3 with structure: machine-learning/{experiment_type}/{model_name}/
    
    Returns dictionary of saved S3 paths
    """
    # Construct S3 paths
    base_path = f"{s3_manager.save_result_path}/machine-learning/{experiment_type}/{model_name}"
    
    s3_paths = {}
    
    # Save results DataFrame
    if results_df is not None and len(results_df) > 0:
        results_path = f"{base_path}/results.pkl"
        s3_manager.save_data_from_path(results_path, results_df, data_format="pkl")
        s3_paths["results"] = results_path
        logger.info(f"‚úÖ Saved results to S3: {results_path}")
    
    # Save summary statistics
    if summary_df is not None and len(summary_df) > 0:
        summary_path = f"{base_path}/summary-stats.csv"
        s3_manager.save_data_from_path(summary_path, summary_df, data_format="csv")
        s3_paths["summary"] = summary_path
        logger.info(f"‚úÖ Saved summary stats to S3: {summary_path}")
    
    # Save metadata
    metadata_path = f"{base_path}/run-metadata.yml"
    s3_manager.save_data_from_path(metadata_path, metadata, data_format="txt")
    s3_paths["metadata"] = metadata_path
    logger.info(f"‚úÖ Saved metadata to S3: {metadata_path}")
    
    # Save failed tasks (if any)
    if failed_tasks_df is not None and len(failed_tasks_df) > 0:
        failed_path = f"{base_path}/failed-tasks.csv"
        s3_manager.save_data_from_path(failed_path, failed_tasks_df, data_format="csv")
        s3_paths["failed_tasks"] = failed_path
        logger.info(f"‚úÖ Saved failed tasks to S3: {failed_path}")
    
    return s3_paths




def main():
    """Main execution function using configuration variables with multi-model support"""
    # Use configuration variables
    csv_path = CSV_PATH
    experiment_types = EXPERIMENT_TYPES
    model_names = MODEL_NAMES
    num_repeats = NUM_REPEATS
    test_size = TEST_SIZE
    random_seed = RANDOM_SEED
    n_jobs = N_JOBS
    upload_s3 = UPLOAD_S3
    skip_failed = SKIP_FAILED
    
    # Normalize model names to list format
    model_names_normalized = normalize_model_names(model_names)
    
    # Send start notification if enabled
    if SEND_NOTIFICATIONS:
        script_name = 'ml-batch-eval'
        extra_info = {
            'CSV': csv_path or 'Auto-discovery',
            'Experiment types': experiment_types,
            'Models': model_names_normalized,
            'Upload S3': upload_s3
        }
        notify_start(script_name, **extra_info)
    
    logger.info("üöÄ Starting ML Batch Evaluation Runner (Configuration Version)")
    logger.info(f"CSV: {csv_path}")
    logger.info(f"Experiment types: {experiment_types}")
    logger.info(f"Upload to S3: {upload_s3}")
    if not upload_s3:
        logger.warning("‚ö†Ô∏è  WARNING: UPLOAD_S3 is False - results will not be saved to S3 or locally")
    logger.info(f"Model names: {model_names_normalized or 'Auto-detect from CSV'}")
    logger.info(f"Num repeats: {num_repeats}, Test size: {test_size}")
    logger.info(f"Random seed: {random_seed}, Parallel jobs: {n_jobs}")
    if SEND_NOTIFICATIONS:
        logger.info("üîî Notifications enabled")
    
    start_time = datetime.now()
    overall_results = {}
    
    try:
        # Initialize s3_manager variable before auto-discovery check
        s3_manager = None
        
        # Load task list
        if csv_path is None:
            # Auto-discovery mode
            logger.info("üöÄ Auto-discovery mode: Finding task lists based on EXPERIMENT_TYPES and MODEL_NAMES")
            
            if upload_s3 is False:
                logger.error("Auto-discovery requires S3 upload (UPLOAD_S3=True)")
                sys.exit(1)
                
            if s3_manager is None:
                s3_manager = S3ConfigManager()
                logger.info("‚úÖ S3 manager initialized for auto-discovery")
            
            # Check that EXPERIMENT_TYPES and MODEL_NAMES are specified
            if experiment_types is None:
                logger.error("Auto-discovery requires EXPERIMENT_TYPES to be specified")
                sys.exit(1)
                
            if model_names_normalized is None:
                logger.error("Auto-discovery requires MODEL_NAMES to be specified")
                sys.exit(1)
            
            task_df = discover_task_lists(experiment_types, model_names_normalized, s3_manager)
            
            if task_df.empty:
                logger.error("No task lists discovered. Please check:")
                logger.error("1. Data-eng scripts were run with GENERATE_ML_TASK_LIST=True")
                logger.error("2. UPLOAD_S3=True in data-eng scripts")
                logger.error("3. EXPERIMENT_TYPES and MODEL_NAMES match generated data")
                logger.error("4. S3 paths are accessible")
                sys.exit(1)
        else:
            # Manual CSV path mode
            logger.info(f"üìÅ Manual mode: Loading CSV from {csv_path}")
            
            if not os.path.exists(csv_path):
                logger.error(f"CSV file not found: {csv_path}")
                sys.exit(1)
            
            task_df = pd.read_csv(csv_path)
        
        # Validate CSV structure
        if not validate_csv_structure(task_df):
            sys.exit(1)
        
        logger.info(f"Loaded CSV with {len(task_df)} tasks")
        
        # Determine models to process (only for manual mode when MODEL_NAMES is None)
        if model_names_normalized is None:
            # Auto-detect: use all unique models in CSV
            model_names_normalized = task_df["model_name"].unique().tolist()
            logger.info(f"Auto-detected models: {model_names_normalized}")
        
        # Initialize S3 manager if needed (only if not already initialized for auto-discovery)
        if s3_manager is None and upload_s3:
            s3_manager = S3ConfigManager()
            logger.info("‚úÖ S3 manager initialized")
        
        # Evaluation parameters
        evaluation_params = {
            "num_repeats": num_repeats,
            "test_size": test_size,
            "random_seed": random_seed,
            "n_jobs": n_jobs
        }
        
        # Process each model
        for model_name in model_names_normalized:
            logger.info(f"\n{'='*60}")
            logger.info(f"PROCESSING MODEL: {model_name}")
            logger.info(f"{'='*60}")
            
            # Filter tasks for this model
            model_tasks = filter_tasks_by_model(task_df, [model_name])
            if len(model_tasks) == 0:
                logger.warning(f"No tasks found for model {model_name}, skipping")
                continue
            
            # No need to filter by experiment_type - CSV already contains tasks for specific experiment
            # Group tasks by experiment type (each CSV already grouped by experiment)
            grouped_tasks = {}
            for exp_type, group_df in model_tasks.groupby("experiment_type"):
                grouped_tasks[exp_type] = group_df.reset_index(drop=True)
                logger.info(f"Experiment '{exp_type}': {len(group_df)} tasks")
            
            # Process each experiment type for this model
            model_results = {}
            model_failed_tasks = []
            
            for experiment_type, exp_task_df in grouped_tasks.items():
                logger.info(f"\n  Processing experiment: {experiment_type}")
                logger.info(f"  {'-'*40}")
                
                # Run batch evaluation
                results_df, failed_tasks_df, metadata = run_batch_evaluation_for_experiment(
                    task_df=exp_task_df,
                    experiment_type=experiment_type,
                    model_name=model_name,
                    s3_manager=s3_manager,
                    evaluation_params=evaluation_params
                )
                
                # Generate summary statistics
                summary_df = None
                if results_df is not None:
                    summary_df = generate_summary_stats(results_df)
                
                # Save results
                if upload_s3 and s3_manager is not None:
                    s3_paths = save_results_to_s3(
                        results_df=results_df,
                        summary_df=summary_df,
                        metadata=metadata,
                        failed_tasks_df=failed_tasks_df,
                        s3_manager=s3_manager,
                        experiment_type=experiment_type,
                        model_name=model_name
                    )
                    metadata["s3_paths"] = s3_paths
                
                # Skip local saving (OUTPUT_DIR removed per user request)
                # Results are only saved to S3 when UPLOAD_S3 = True
                
                # Store results
                model_results[experiment_type] = {
                    "results": results_df,
                    "summary": summary_df,
                    "metadata": metadata
                }
                
                if failed_tasks_df is not None:
                    model_failed_tasks.append(failed_tasks_df)
                
                logger.info(f"  ‚úÖ Completed experiment: {experiment_type}")
            
            # Store model results
            overall_results[model_name] = {
                "experiment_results": model_results,
                "failed_tasks": model_failed_tasks,
                "total_experiments": len(model_results)
            }
            
            logger.info(f"‚úÖ Completed processing for model: {model_name}")
        
        # Calculate execution time
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Print summary
        logger.info(f"\n{'='*60}")
        logger.info("üìä EXECUTION SUMMARY")
        logger.info(f"{'='*60}")
        logger.info(f"Total models processed: {len(overall_results)}")
        logger.info(f"Total execution time: {duration:.2f} seconds ({duration/60:.2f} minutes)")
        
        successful_models = []
        failed_models = []
        
        for model_name, model_data in overall_results.items():
            successful_exps = [exp for exp, data in model_data["experiment_results"].items() if data["results"] is not None]
            failed_exps = [exp for exp, data in model_data["experiment_results"].items() if data["results"] is None]
            
            if failed_exps:
                failed_models.append(model_name)
            else:
                successful_models.append(model_name)
            
            logger.info(f"\n  Model: {model_name}")
            logger.info(f"    Experiments: {len(model_data['experiment_results'])} total")
            logger.info(f"    Successful: {len(successful_exps)}")
            if failed_exps:
                logger.info(f"    Failed: {len(failed_exps)} - {', '.join(failed_exps)}")
        
        logger.info(f"\n‚úÖ Successful models: {len(successful_models)}")
        if successful_models:
            logger.info(f"  - {', '.join(successful_models)}")
        
        if failed_models:
            logger.info(f"‚ùå Models with failures: {len(failed_models)}")
            logger.info(f"  - {', '.join(failed_models)}")
        
        # Skip overall summary saving (OUTPUT_DIR removed per user request)
        
        logger.info("üéâ ML Batch Evaluation completed successfully!")
        
        # Send success notification if enabled
        if SEND_NOTIFICATIONS:
            processed_count = len(overall_results)
            notify_success('ml-batch-eval', duration, processed_count=processed_count)
        
    except Exception as e:
        logger.error(f"‚ùå ML Batch Evaluation failed: {e}")
        logger.error(traceback.format_exc())
        
        # Send failure notification if enabled
        if SEND_NOTIFICATIONS:
            duration = (datetime.now() - start_time).total_seconds()
            notify_failure('ml-batch-eval', e, duration_seconds=duration)
        
        sys.exit(1)


if __name__ == "__main__":
    main()
