"""
ML Baseline Evaluation Runner - Configuration Version
For remote batch job execution on baseline dynamics data

Loads CSV task lists generated by generate-baseline-dynamics-v1.py, runs batch ML evaluation,
and saves results to S3 under machine-learning/baseline/{model}/ directory.

S3 Output Structure:
{save_result_path}/machine-learning/baseline/{model_name}/
‚îú‚îÄ‚îÄ results.pkl            # Full evaluation results DataFrame
‚îú‚îÄ‚îÄ summary-stats.csv      # Aggregated statistics
‚îú‚îÄ‚îÄ run-metadata.yml       # Run configuration and metadata
‚îî‚îÄ‚îÄ failed-tasks.csv       # Tasks that failed during processing (if any)

CONFIGURATION INSTRUCTIONS:
1. Modify the configuration variables below
2. Run the script: python run-ml-baseline-v1.py
3. For different runs, copy this script and modify the configuration
"""

import sys
import os
import pandas as pd
import numpy as np
import yaml
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
import traceback

# ===== CONFIGURATION =====
# Modify these variables for your batch job

# CSV file configuration
CSV_PATH = None  # Set to None for auto-discovery, or provide custom path: "path/to/your/tasks.csv"
# When CSV_PATH is None, the script will auto-discover task lists based on MODEL_NAMES
# When CSV_PATH is provided, the script will load the specified CSV file

# Model configuration
MODEL_NAMES = ["sy_simple"]  # List of model names to process
# Options:
# MODEL_NAMES = ["sy_simple"]  # Single model
# MODEL_NAMES = ["sy_simple", "v1", "fgfr4_model"]  # Multiple models
# IMPORTANT: For auto-discovery (CSV_PATH=None), MODEL_NAMES must be specified as a list

# Evaluation parameters
NUM_REPEATS = 10     # Number of random train/test splits
TEST_SIZE = 0.2      # Proportion of data for testing
RANDOM_SEED = 42     # Random seed for reproducibility
N_JOBS = -1          # Number of parallel jobs (-1 for all cores)
CLIP_THRESHOLD = 1e9 # Clip feature values to ¬±threshold to prevent float32 overflow

# Output configuration
UPLOAD_S3 = True     # Upload results to S3 (True/False)
SKIP_FAILED = True   # Skip failed tasks and continue processing

# Notification configuration (matching data-eng scripts)
SEND_NOTIFICATIONS = True   # Send ntfy notifications (True/False)

# ===== END CONFIGURATION =====

# Add src to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, "../../..")
sys.path.insert(0, src_dir)

from models.utils.s3_config_manager import S3ConfigManager
from scripts.ntfy_notifier import notify_start, notify_success, notify_failure

# Import BatchLoader from create-ml-loader-v1.py
import importlib.util

# Helper to import from hyphenated file
def import_from_hyphenated_file(filepath, class_name):
    """Import a class from a Python file with hyphenated name"""
    module_name = filepath.replace('-', '_').replace('.py', '')
    file_dir = os.path.dirname(filepath)
    
    # Add the file's directory to sys.path so it can find local imports
    original_sys_path = sys.path.copy()
    if file_dir not in sys.path:
        sys.path.insert(0, file_dir)
    
    try:
        spec = importlib.util.spec_from_file_location(module_name, filepath)
        module = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = module
        spec.loader.exec_module(module)
        return getattr(module, class_name)
    finally:
        # Restore original sys.path
        sys.path = original_sys_path

# Import BatchLoader from the create-ml-loader-v1.py file
create_loader_path = os.path.join(current_dir, "../data-eng/create-ml-loader-v1.py")
BatchLoader = import_from_hyphenated_file(create_loader_path, "BatchLoader")

# Import batch_eval_standard from ml.Workflow
ml_dir = os.path.join(src_dir, "ml")
sys.path.insert(0, ml_dir)

try:
    from Workflow import batch_eval_standard
except ImportError:
    # Try alternative import path
    sys.path.insert(0, os.path.join(src_dir, "ml"))
    import Workflow
    batch_eval_standard = Workflow.batch_eval_standard

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def discover_task_lists(model_names, s3_manager):
    """
    Discover task list CSVs for baseline dynamics based on MODEL_NAMES
    
    Looks for CSVs at: {s3_path}/data/{model_name}_baseline_dynamics_v1/task_list.csv
    Returns: Combined DataFrame of all discovered tasks
    """
    all_tasks = []
    
    for model_name in model_names:
        # Construct expected CSV path based on pattern from baseline-dynamics script
        folder_name = f"{model_name}_baseline_dynamics_v1"
        csv_path = f"{s3_manager.save_result_path}/data/{folder_name}/task_list.csv"
        
        try:
            logger.info(f"Looking for task list: {csv_path}")
            task_df = s3_manager.load_data_from_path(csv_path, data_format="csv")
            
            if not task_df.empty:
                all_tasks.append(task_df)
                logger.info(f"Found {len(task_df)} tasks for {model_name}")
            else:
                logger.warning(f"CSV is empty for {model_name}")
                
        except Exception as e:
            logger.warning(f"Task list not found or error loading: {csv_path} - {str(e)[:100]}")
    
    if all_tasks:
        combined_df = pd.concat(all_tasks, ignore_index=True)
        logger.info(f"Discovered total of {len(combined_df)} tasks across all models")
        return combined_df
    else:
        logger.error("No task lists discovered for the specified models")
        return pd.DataFrame()


def validate_csv_structure(df: pd.DataFrame) -> bool:
    """Validate CSV has required columns"""
    required_columns = [
        "feature_data", "feature_data_label", 
        "target_data", "target_data_label",
        "experiment_type", "level", "model_name"
    ]
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logger.error(f"CSV missing required columns: {missing_columns}")
        return False
    
    return True


def normalize_model_names(model_names):
    """Convert model_names configuration to list format"""
    if model_names is None:
        return None
    elif isinstance(model_names, str):
        return [model_names]
    elif isinstance(model_names, list):
        return model_names
    else:
        logger.warning(f"Unexpected model_names type: {type(model_names)}, converting to list")
        return [str(model_names)]


def filter_tasks_by_model(df: pd.DataFrame, model_names: Optional[List[str]]) -> pd.DataFrame:
    """Filter tasks by model name(s)"""
    if model_names is None:
        return df
    
    filtered_df = df[df["model_name"].isin(model_names)].copy()
    logger.info(f"Filtered to {len(filtered_df)} tasks from {len(df)} total for models: {model_names}")
    
    return filtered_df


def run_batch_evaluation_for_model(
    task_df: pd.DataFrame,
    model_name: str,
    s3_manager: S3ConfigManager,
    evaluation_params: Dict[str, Any]
) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Dict[str, Any]]:
    """
    Run batch evaluation for a specific model
    
    Returns:
        Tuple of (results_df, failed_tasks_df, metadata)
    """
    logger.info(f"Starting batch evaluation for model: {model_name}")
    
    # Initialize BatchLoader
    loader = BatchLoader(s3_manager=s3_manager)
    
    # Create temporary CSV for this model
    temp_csv = f"temp_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    task_df.to_csv(temp_csv, index=False)
    
    try:
        # Load task list
        loader.load_task_list(temp_csv)
        
        # Prepare data for batch_eval
        feature_data_list, feature_data_names, target_data, target_name = loader.prepare_for_batch_eval()
        
        logger.info(f"Prepared {len(feature_data_list)} feature datasets for evaluation")
        logger.info(f"Target column: {target_name}")
        
        # Run batch evaluation
        results_df = batch_eval_standard(
            feature_data_list=feature_data_list,
            feature_data_names=feature_data_names,
            target_data=target_data,
            target_name=target_name,
            num_repeats=evaluation_params["num_repeats"],
            test_size=evaluation_params["test_size"],
            o_random_seed=evaluation_params["random_seed"],
            n_jobs=evaluation_params["n_jobs"],
            clip_threshold=evaluation_params.get("clip_threshold", 1e9)
        )
        
        # Add experiment metadata to results
        results_df["experiment_type"] = "baseline-dynamics-v1"
        results_df["model_name"] = model_name
        results_df["evaluation_timestamp"] = datetime.now().isoformat()
        
        logger.info(f"Batch evaluation completed: {len(results_df)} results generated")
        
        # Generate metadata
        metadata = {
            "experiment_type": "baseline-dynamics-v1",
            "model_name": model_name,
            "evaluation_params": evaluation_params,
            "task_stats": {
                "total_tasks": len(task_df),
                "successful_tasks": len(task_df),  # All succeeded if we got here
                "failed_tasks": 0
            },
            "results_stats": {
                "total_results": len(results_df),
                "unique_models": results_df["Model"].nunique(),
                "unique_features": results_df["Feature Data"].nunique(),
                "mean_r2": results_df["R2 Score"].mean(),
                "mean_mse": results_df["Mean Squared Error"].mean()
            },
            "timestamp": datetime.now().isoformat(),
            "script_version": "run-ml-baseline-v1.py"
        }
        
        failed_tasks_df = None
        
    except Exception as e:
        logger.error(f"Batch evaluation failed for model {model_name}: {e}")
        logger.error(traceback.format_exc())
        
        results_df = None
        metadata = {
            "experiment_type": "baseline-dynamics-v1",
            "model_name": model_name,
            "error": str(e),
            "error_traceback": traceback.format_exc(),
            "timestamp": datetime.now().isoformat(),
            "status": "failed"
        }
        
        # Create failed tasks DataFrame
        failed_tasks_df = task_df.copy()
        failed_tasks_df["error"] = str(e)
        failed_tasks_df["failed_timestamp"] = datetime.now().isoformat()
        
    finally:
        # Clean up temporary file
        if os.path.exists(temp_csv):
            os.remove(temp_csv)
    
    return results_df, failed_tasks_df, metadata


def generate_summary_stats(results_df: pd.DataFrame) -> pd.DataFrame:
    """Generate summary statistics from results"""
    if results_df is None or len(results_df) == 0:
        return pd.DataFrame()
    
    # Group by Model and Feature Data
    summary = results_df.groupby(["Model", "Feature Data"]).agg({
        "R2 Score": ["mean", "std", "min", "max"],
        "Mean Squared Error": ["mean", "std", "min", "max"],
        "Pearson Correlation": ["mean", "std", "min", "max"]
    }).round(4)
    
    # Flatten column names
    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]
    summary = summary.reset_index()
    
    return summary


def save_results_to_s3(
    results_df: pd.DataFrame,
    summary_df: pd.DataFrame,
    metadata: Dict[str, Any],
    failed_tasks_df: Optional[pd.DataFrame],
    s3_manager: S3ConfigManager,
    model_name: str
) -> Dict[str, str]:
    """
    Save results to S3 with structure: machine-learning/baseline/{model_name}/
    
    Returns dictionary of saved S3 paths
    """
    # Construct S3 paths
    base_path = f"{s3_manager.save_result_path}/machine-learning/baseline/{model_name}"
    
    s3_paths = {}
    
    # Save results DataFrame
    if results_df is not None and len(results_df) > 0:
        results_path = f"{base_path}/results.pkl"
        s3_manager.save_data_from_path(results_path, results_df, data_format="pkl")
        s3_paths["results"] = results_path
        logger.info(f"‚úÖ Saved results to S3: {results_path}")
    
    # Save summary statistics
    if summary_df is not None and len(summary_df) > 0:
        summary_path = f"{base_path}/summary-stats.csv"
        s3_manager.save_data_from_path(summary_path, summary_df, data_format="csv")
        s3_paths["summary"] = summary_path
        logger.info(f"‚úÖ Saved summary stats to S3: {summary_path}")
    
    # Save metadata
    metadata_path = f"{base_path}/run-metadata.yml"
    s3_manager.save_data_from_path(metadata_path, metadata, data_format="txt")
    s3_paths["metadata"] = metadata_path
    logger.info(f"‚úÖ Saved metadata to S3: {metadata_path}")
    
    # Save failed tasks (if any)
    if failed_tasks_df is not None and len(failed_tasks_df) > 0:
        failed_path = f"{base_path}/failed-tasks.csv"
        s3_manager.save_data_from_path(failed_path, failed_tasks_df, data_format="csv")
        s3_paths["failed_tasks"] = failed_path
        logger.info(f"‚úÖ Saved failed tasks to S3: {failed_path}")
    
    return s3_paths


def main():
    """Main execution function using configuration variables with multi-model support"""
    # Use configuration variables
    csv_path = CSV_PATH
    model_names = MODEL_NAMES
    num_repeats = NUM_REPEATS
    test_size = TEST_SIZE
    random_seed = RANDOM_SEED
    n_jobs = N_JOBS
    clip_threshold = CLIP_THRESHOLD
    upload_s3 = UPLOAD_S3
    skip_failed = SKIP_FAILED
    
    # Normalize model names to list format
    model_names_normalized = normalize_model_names(model_names)
    
    # Send start notification if enabled
    if SEND_NOTIFICATIONS:
        script_name = 'ml-baseline-eval'
        extra_info = {
            'CSV': csv_path or 'Auto-discovery',
            'Models': model_names_normalized,
            'Upload S3': upload_s3
        }
        notify_start(script_name, **extra_info)
    
    logger.info("üöÄ Starting ML Baseline Evaluation Runner (Configuration Version)")
    logger.info(f"CSV: {csv_path}")
    logger.info(f"Models: {model_names_normalized}")
    logger.info(f"Upload to S3: {upload_s3}")
    if not upload_s3:
        logger.warning("‚ö†Ô∏è  WARNING: UPLOAD_S3 is False - results will not be saved to S3 or locally")
    logger.info(f"Num repeats: {num_repeats}, Test size: {test_size}")
    logger.info(f"Random seed: {random_seed}, Parallel jobs: {n_jobs}")
    logger.info(f"Clip threshold: {clip_threshold}")
    if SEND_NOTIFICATIONS:
        logger.info("üîî Notifications enabled")
    
    start_time = datetime.now()
    overall_results = {}
    
    try:
        # Initialize s3_manager variable before auto-discovery check
        s3_manager = None
        
        # Load task list
        if csv_path is None:
            # Auto-discovery mode
            logger.info("üöÄ Auto-discovery mode: Finding task lists based on MODEL_NAMES")
            
            if upload_s3 is False:
                logger.error("Auto-discovery requires S3 upload (UPLOAD_S3=True)")
                sys.exit(1)
                
            if s3_manager is None:
                s3_manager = S3ConfigManager()
                logger.info("‚úÖ S3 manager initialized for auto-discovery")
            
            # Check that MODEL_NAMES are specified
            if model_names_normalized is None:
                logger.error("Auto-discovery requires MODEL_NAMES to be specified")
                sys.exit(1)
            
            task_df = discover_task_lists(model_names_normalized, s3_manager)
            
            if task_df.empty:
                logger.error("No task lists discovered. Please check:")
                logger.error("1. generate-baseline-dynamics-v1.py was run with GENERATE_ML_TASK_LIST=True")
                logger.error("2. UPLOAD_S3=True in baseline-dynamics script")
                logger.error("3. MODEL_NAMES match generated data")
                logger.error("4. S3 paths are accessible")
                sys.exit(1)
        else:
            # Manual CSV path mode
            logger.info(f"üìÅ Manual mode: Loading CSV from {csv_path}")
            
            if not os.path.exists(csv_path):
                logger.error(f"CSV file not found: {csv_path}")
                sys.exit(1)
            
            task_df = pd.read_csv(csv_path)
        
        # Validate CSV structure
        if not validate_csv_structure(task_df):
            sys.exit(1)
        
        logger.info(f"Loaded CSV with {len(task_df)} tasks")
        
        # Determine models to process (only for manual mode when MODEL_NAMES is None)
        if model_names_normalized is None:
            # Auto-detect: use all unique models in CSV
            model_names_normalized = task_df["model_name"].unique().tolist()
            logger.info(f"Auto-detected models: {model_names_normalized}")
        
        # Initialize S3 manager if needed (only if not already initialized for auto-discovery)
        if s3_manager is None and upload_s3:
            s3_manager = S3ConfigManager()
            logger.info("‚úÖ S3 manager initialized")
        
        # Evaluation parameters
        evaluation_params = {
            "num_repeats": num_repeats,
            "test_size": test_size,
            "random_seed": random_seed,
            "n_jobs": n_jobs,
            "clip_threshold": clip_threshold
        }
        
        # Process each model
        for model_name in model_names_normalized:
            logger.info(f"\n{'='*60}")
            logger.info(f"PROCESSING MODEL: {model_name}")
            logger.info(f"{'='*60}")
            
            # Filter tasks for this model
            model_tasks = filter_tasks_by_model(task_df, [model_name])
            if len(model_tasks) == 0:
                logger.warning(f"No tasks found for model {model_name}, skipping")
                continue
            
            # Run batch evaluation for this model
            results_df, failed_tasks_df, metadata = run_batch_evaluation_for_model(
                task_df=model_tasks,
                model_name=model_name,
                s3_manager=s3_manager,
                evaluation_params=evaluation_params
            )
            
            # Generate summary statistics
            summary_df = None
            if results_df is not None:
                summary_df = generate_summary_stats(results_df)
            
            # Save results
            if upload_s3 and s3_manager is not None:
                s3_paths = save_results_to_s3(
                    results_df=results_df,
                    summary_df=summary_df,
                    metadata=metadata,
                    failed_tasks_df=failed_tasks_df,
                    s3_manager=s3_manager,
                    model_name=model_name
                )
                metadata["s3_paths"] = s3_paths
            
            # Store model results
            overall_results[model_name] = {
                "results": results_df,
                "summary": summary_df,
                "metadata": metadata,
                "failed_tasks": failed_tasks_df
            }
            
            logger.info(f"‚úÖ Completed processing for model: {model_name}")
        
        # Calculate execution time
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Print summary
        logger.info(f"\n{'='*60}")
        logger.info("üìä EXECUTION SUMMARY")
        logger.info(f"{'='*60}")
        logger.info(f"Total models processed: {len(overall_results)}")
        logger.info(f"Total execution time: {duration:.2f} seconds ({duration/60:.2f} minutes)")
        
        successful_models = []
        failed_models = []
        
        for model_name, model_data in overall_results.items():
            if model_data["results"] is not None:
                successful_models.append(model_name)
            else:
                failed_models.append(model_name)
            
            logger.info(f"\n  Model: {model_name}")
            logger.info(f"    Status: {'‚úÖ Success' if model_data['results'] is not None else '‚ùå Failed'}")
            if model_data["results"] is not None:
                logger.info(f"    Results: {len(model_data['results'])} rows")
                logger.info(f"    Mean R¬≤: {model_data['results']['R2 Score'].mean():.4f}")
        
        logger.info(f"\n‚úÖ Successful models: {len(successful_models)}")
        if successful_models:
            logger.info(f"  - {', '.join(successful_models)}")
        
        if failed_models:
            logger.info(f"‚ùå Models with failures: {len(failed_models)}")
            logger.info(f"  - {', '.join(failed_models)}")
        
        logger.info("üéâ ML Baseline Evaluation completed successfully!")
        
        # Send success notification if enabled
        if SEND_NOTIFICATIONS:
            processed_count = len(overall_results)
            notify_success('ml-baseline-eval', duration, processed_count=processed_count)
        
    except Exception as e:
        logger.error(f"‚ùå ML Baseline Evaluation failed: {e}")
        logger.error(traceback.format_exc())
        
        # Send failure notification if enabled
        if SEND_NOTIFICATIONS:
            duration = (datetime.now() - start_time).total_seconds()
            notify_failure('ml-baseline-eval', e, duration_seconds=duration)
        
        sys.exit(1)


if __name__ == "__main__":
    main()
